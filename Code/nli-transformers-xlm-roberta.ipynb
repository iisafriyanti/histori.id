{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":21733,"databundleVersionId":1408234,"sourceType":"competition"},{"sourceId":10064578,"sourceType":"datasetVersion","datasetId":6202578}],"dockerImageVersionId":30528,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# NLI Problem Solve\nNatural Language Inferencing (NLI) is a classic NLP (Natural Language Processing) problem that involves taking two sentences (the premise and the hypothesis ), and deciding how they are related- if the premise entails the hypothesis, contradicts it, or neither.","metadata":{}},{"cell_type":"code","source":"!pip install ipywidgets\n!jupyter nbextension enable --py widgetsnbextension","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T09:28:20.714509Z","iopub.execute_input":"2024-12-02T09:28:20.714832Z","iopub.status.idle":"2024-12-02T09:28:30.234958Z","shell.execute_reply.started":"2024-12-02T09:28:20.714805Z","shell.execute_reply":"2024-12-02T09:28:30.233894Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.10/site-packages (7.7.1)\nRequirement already satisfied: ipykernel>=4.5.1 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (6.23.3)\nRequirement already satisfied: ipython-genutils~=0.2.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (0.2.0)\nRequirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (5.9.0)\nRequirement already satisfied: widgetsnbextension~=3.6.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (3.6.4)\nRequirement already satisfied: ipython>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (8.14.0)\nRequirement already satisfied: jupyterlab-widgets>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (3.0.7)\nRequirement already satisfied: comm>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.3)\nRequirement already satisfied: debugpy>=1.6.5 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.7)\nRequirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (7.4.9)\nRequirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (5.3.1)\nRequirement already satisfied: matplotlib-inline>=0.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.6)\nRequirement already satisfied: nest-asyncio in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.6)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.3)\nRequirement already satisfied: pyzmq>=20 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (25.1.0)\nRequirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.3.2)\nRequirement already satisfied: backcall in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\nRequirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets) (5.1.1)\nRequirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets) (0.18.2)\nRequirement already satisfied: pickleshare in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\nRequirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets) (3.0.38)\nRequirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets) (2.15.1)\nRequirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets) (0.6.2)\nRequirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets) (4.8.0)\nRequirement already satisfied: notebook>=4.4.1 in /opt/conda/lib/python3.10/site-packages (from widgetsnbextension~=3.6.0->ipywidgets) (6.5.4)\nRequirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.3)\nRequirement already satisfied: entrypoints in /opt/conda/lib/python3.10/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (0.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.8.2)\nRequirement already satisfied: platformdirs>=2.5 in /opt/conda/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=4.5.1->ipywidgets) (3.8.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.2)\nRequirement already satisfied: argon2-cffi in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.3.0)\nRequirement already satisfied: nbformat in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.9.0)\nRequirement already satisfied: nbconvert>=5 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.4.5)\nRequirement already satisfied: Send2Trash>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.2)\nRequirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.17.1)\nRequirement already satisfied: prometheus-client in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.17.0)\nRequirement already satisfied: nbclassic>=0.4.7 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.0.0)\nRequirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=4.0.0->ipywidgets) (0.2.6)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->ipykernel>=4.5.1->ipywidgets) (3.0.9)\nRequirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=4.0.0->ipywidgets) (1.2.0)\nRequirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=4.0.0->ipywidgets) (2.2.1)\nRequirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=4.0.0->ipywidgets) (0.2.2)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from asttokens>=2.1.0->stack-data->ipython>=4.0.0->ipywidgets) (1.16.0)\nRequirement already satisfied: jupyter-server>=1.8 in /opt/conda/lib/python3.10/site-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.6.0)\nRequirement already satisfied: notebook-shim>=0.2.3 in /opt/conda/lib/python3.10/site-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.2.3)\nRequirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.8.4)\nRequirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.2.2)\nRequirement already satisfied: bleach in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.0.0)\nRequirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.0)\nRequirement already satisfied: testpath in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.6.0)\nRequirement already satisfied: defusedxml in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.1)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.12.2)\nRequirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.13)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.1.3)\nRequirement already satisfied: fastjsonschema in /opt/conda/lib/python3.10/site-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.17.1)\nRequirement already satisfied: jsonschema>=2.6 in /opt/conda/lib/python3.10/site-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.17.3)\nRequirement already satisfied: argon2-cffi-bindings in /opt/conda/lib/python3.10/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.2.0)\nRequirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (23.1.0)\nRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.19.3)\nRequirement already satisfied: anyio>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.7.0)\nRequirement already satisfied: jupyter-events>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.6.3)\nRequirement already satisfied: jupyter-server-terminals in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.4.4)\nRequirement already satisfied: overrides in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.5.0)\nRequirement already satisfied: websocket-client in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.6.0)\nRequirement already satisfied: cffi>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.15.1)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.3.2.post1)\nRequirement already satisfied: webencodings in /opt/conda/lib/python3.10/site-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.1)\nRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.4)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.0)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.1.1)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.21)\nRequirement already satisfied: python-json-logger>=2.0.4 in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.6.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.0.7)\nRequirement already satisfied: pyyaml>=5.3 in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.6.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.0)\nRequirement already satisfied: rfc3339-validator in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.6.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.1.4)\nRequirement already satisfied: rfc3986-validator>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.6.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.1.1)\nRequirement already satisfied: fqdn in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.1)\nRequirement already satisfied: isoduration in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (20.11.0)\nRequirement already satisfied: jsonpointer>1.13 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.0)\nRequirement already satisfied: uri-template in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.0)\nRequirement already satisfied: webcolors>=1.11 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.13)\nRequirement already satisfied: arrow>=0.15.0 in /opt/conda/lib/python3.10/site-packages (from isoduration->jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.2.3)\nConfig option `kernel_spec_manager_class` not recognized by `EnableNBExtensionApp`.\nEnabling notebook extension jupyter-js-widgets/extension...\n      - Validating: \u001b[32mOK\u001b[0m\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# from transformers import BertTokenizer, TFBertModel\nimport matplotlib.pyplot as plt\nimport tensorflow as tf","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-12-02T09:28:37.901864Z","iopub.execute_input":"2024-12-02T09:28:37.902603Z","iopub.status.idle":"2024-12-02T09:28:37.907152Z","shell.execute_reply.started":"2024-12-02T09:28:37.902571Z","shell.execute_reply":"2024-12-02T09:28:37.906205Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# set GPU\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n        strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")  # Создаем стратегию для одного GPU\n    except RuntimeError as e:\n        print(e)\nelse:\n    strategy = tf.distribute.get_strategy()\n    print('Number of replicas:', strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2024-12-02T09:28:41.681735Z","iopub.execute_input":"2024-12-02T09:28:41.682385Z","iopub.status.idle":"2024-12-02T09:28:44.407458Z","shell.execute_reply.started":"2024-12-02T09:28:41.682358Z","shell.execute_reply":"2024-12-02T09:28:44.406456Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_API_KEY\"] = \"0\" ## to silence warning","metadata":{"execution":{"iopub.status.busy":"2024-12-02T09:28:49.270935Z","iopub.execute_input":"2024-12-02T09:28:49.271999Z","iopub.status.idle":"2024-12-02T09:28:49.275583Z","shell.execute_reply.started":"2024-12-02T09:28:49.271966Z","shell.execute_reply":"2024-12-02T09:28:49.274796Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# Fast EDA","metadata":{}},{"cell_type":"code","source":"train = pd.read_json(\"../input/dataset-indonli-new/train.jsonl\", lines=True)\ntest = pd.read_csv(\"../input/contradictory-my-dear-watson/test.csv\")\n\ntrain[\"label\"] = train['label'].replace({'c':2,'e':0,'n':1})\n\ntrain.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T09:28:56.241704Z","iopub.execute_input":"2024-12-02T09:28:56.242025Z","iopub.status.idle":"2024-12-02T09:28:56.437398Z","shell.execute_reply.started":"2024-12-02T09:28:56.242000Z","shell.execute_reply":"2024-12-02T09:28:56.436378Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"   pair_id  premise_id                                            premise  \\\n0   101315       10131  Presiden Joko Widodo (Jokowi) menyampaikan pre...   \n1   110511       11051  Meski biasanya hanya digunakan di fasilitas ke...   \n2   124434       12443  Data dari Nielsen Music mencatat, \"Joanne\" tel...   \n3   124274       12427  Album Wild West miliknya pada tahun 1981 merup...   \n4   119442       11944  Seperti namanya, paket internet sahur Telkomse...   \n\n                                          hypothesis  label data_split  \\\n0     Prediksi akhir wabah tidak disampaikan Jokowi.      2      train   \n1  Masker sekali pakai banyak dipakai di tingkat ...      0      train   \n2      Nielsen Music mencatat pada akhir minggu ini.      1      train   \n3                 Ia memiliki lebih dari satu album.      1      train   \n4  Paket internet sahur tidak ditujukan untuk saa...      2      train   \n\n  annotator_type sentence_size  \n0            lay        single  \n1            lay        single  \n2            lay        single  \n3            lay        single  \n4            lay        single  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pair_id</th>\n      <th>premise_id</th>\n      <th>premise</th>\n      <th>hypothesis</th>\n      <th>label</th>\n      <th>data_split</th>\n      <th>annotator_type</th>\n      <th>sentence_size</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>101315</td>\n      <td>10131</td>\n      <td>Presiden Joko Widodo (Jokowi) menyampaikan pre...</td>\n      <td>Prediksi akhir wabah tidak disampaikan Jokowi.</td>\n      <td>2</td>\n      <td>train</td>\n      <td>lay</td>\n      <td>single</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>110511</td>\n      <td>11051</td>\n      <td>Meski biasanya hanya digunakan di fasilitas ke...</td>\n      <td>Masker sekali pakai banyak dipakai di tingkat ...</td>\n      <td>0</td>\n      <td>train</td>\n      <td>lay</td>\n      <td>single</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>124434</td>\n      <td>12443</td>\n      <td>Data dari Nielsen Music mencatat, \"Joanne\" tel...</td>\n      <td>Nielsen Music mencatat pada akhir minggu ini.</td>\n      <td>1</td>\n      <td>train</td>\n      <td>lay</td>\n      <td>single</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>124274</td>\n      <td>12427</td>\n      <td>Album Wild West miliknya pada tahun 1981 merup...</td>\n      <td>Ia memiliki lebih dari satu album.</td>\n      <td>1</td>\n      <td>train</td>\n      <td>lay</td>\n      <td>single</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>119442</td>\n      <td>11944</td>\n      <td>Seperti namanya, paket internet sahur Telkomse...</td>\n      <td>Paket internet sahur tidak ditujukan untuk saa...</td>\n      <td>2</td>\n      <td>train</td>\n      <td>lay</td>\n      <td>single</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"#train = pd.read_csv(\"../input/contradictory-my-dear-watson/train.csv\")\n#test = pd.read_csv(\"../input/contradictory-my-dear-watson/test.csv\")\n\n#train.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2024-12-02T09:29:02.539978Z","iopub.execute_input":"2024-12-02T09:29:02.540332Z","iopub.status.idle":"2024-12-02T09:29:02.572375Z","shell.execute_reply.started":"2024-12-02T09:29:02.540305Z","shell.execute_reply":"2024-12-02T09:29:02.571562Z"},"trusted":true},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 10330 entries, 0 to 10329\nData columns (total 8 columns):\n #   Column          Non-Null Count  Dtype \n---  ------          --------------  ----- \n 0   pair_id         10330 non-null  int64 \n 1   premise_id      10330 non-null  int64 \n 2   premise         10330 non-null  object\n 3   hypothesis      10330 non-null  object\n 4   label           10330 non-null  int64 \n 5   data_split      10330 non-null  object\n 6   annotator_type  10330 non-null  object\n 7   sentence_size   10330 non-null  object\ndtypes: int64(3), object(5)\nmemory usage: 645.8+ KB\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"!pip install plotly","metadata":{"execution":{"iopub.status.busy":"2024-12-02T09:30:13.786401Z","iopub.execute_input":"2024-12-02T09:30:13.787258Z","iopub.status.idle":"2024-12-02T09:30:21.454633Z","shell.execute_reply.started":"2024-12-02T09:30:13.787231Z","shell.execute_reply":"2024-12-02T09:30:21.453677Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: plotly in /opt/conda/lib/python3.10/site-packages (5.15.0)\nRequirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from plotly) (8.2.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from plotly) (21.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->plotly) (3.0.9)\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"#Cannot be run using dataset indonli\nimport plotly.express as px\n\nlabels, frequencies = np.unique(train.language.values, return_counts = True)\n\nfig = px.pie(values=frequencies, \n             names=labels, \n             title='Languages distribution',\n             color_discrete_sequence=px.colors.sequential.Plotly3)\n\nfig.show(renderer=\"iframe\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import plotly.graph_objects as go\n\ntrain['text_length'] = train['premise'].apply(len)\n\nfig = go.Figure(data=[go.Histogram(x=train['text_length'], \n                                   nbinsx=50,\n                                   marker_color='skyblue')])\nfig.update_layout(title_text='Text length distribution in \"premise\"', # title of plot\n                  xaxis_title_text='Len of text in premise', # xaxis label\n                  yaxis_title_text='Number of sentences', # yaxis label\n                  bargap=0.2, # gap between bars of adjacent location coordinates\n                  bargroupgap=0.1) # gap between bars of the same location coordinates\nfig.show(renderer=\"iframe\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train['text_length'] = train['hypothesis'].apply(len)\n\nfig = go.Figure(data=[go.Histogram(x=train['text_length'], \n                                   nbinsx=50,\n                                   marker_color='skyblue')])\nfig.update_layout(title_text='Text length distribution in \"hypothesis\"', # title of plot\n                  xaxis_title_text='Len of text in hypothesis', # xaxis label\n                  yaxis_title_text='Number of sentences', # yaxis label\n                  bargap=0.2, # gap between bars of adjacent location coordinates\n                  bargroupgap=0.1) # gap between bars of the same location coordinates\nfig.show(renderer=\"iframe\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import plotly.graph_objects as go\n\nlabel_count = train['label'].value_counts().sort_index()\nlabel_names = ['entailment', 'neutral', 'contradiction']\nlabel_count.index = label_names\n\nfig = go.Figure([go.Bar(x=label_names, y=label_count, marker_color='skyblue')])\n\nfig.update_layout(title_text='Number of entries per label', # title of plot\n                  xaxis_title_text='Label', # xaxis label\n                  yaxis_title_text='Count', # yaxis label\n                  )\nfig.show(renderer=\"iframe\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T09:31:29.586420Z","iopub.execute_input":"2024-12-02T09:31:29.587034Z","iopub.status.idle":"2024-12-02T09:31:29.932492Z","shell.execute_reply.started":"2024-12-02T09:31:29.587003Z","shell.execute_reply":"2024-12-02T09:31:29.931680Z"}},"outputs":[{"output_type":"display_data","data":{"text/html":"<iframe\n    scrolling=\"no\"\n    width=\"100%\"\n    height=\"545px\"\n    src=\"iframe_figures/figure_15.html\"\n    frameborder=\"0\"\n    allowfullscreen\n></iframe>\n"},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"# Transformers 🤖","metadata":{}},{"cell_type":"markdown","source":"In this notebook we will be use XLM-RoBERTa. <br><br>\nOur 1st step is to import \"symanto/xlm-roberta-base-snli-mnli-anli-xnli\". So this is a pre-trained model based on the XLM-RoBERTa architecture, which has been further fine-tuned on the SNLI, MNLI, ANLI, and XNLI datasets. Let's unpack what these abbreviations mean:\n\n1. XLM-RoBERTa: XLM-RoBERTa (Cross-lingual Language Model - RoBERTa) is a variant of the RoBERTa model that is designed to work with texts in multiple languages. It was developed by the Facebook AI team and trained on a large corpus of text from 100 languages.\n\n2. SNLI: The Stanford Natural Language Inference Corpus is a dataset for the task of natural language inference (NLI), consisting of sentences annotated for 'entailment', 'contradiction', or 'neutrality' relations.\n\n3. MNLI: The Multi-Genre Natural Language Inference Corpus is another dataset for the NLI task, which incorporates a variety of genres and text sources.\n\n4. ANLI: The Adversarial Natural Language Inference task is a dataset consisting of several 'rounds' of NLI tasks, each progressively more difficult.\n\n5. XNLI: The Cross-lingual Natural Language Inference Corpus is a multilingual dataset for the NLI task, based on MNLI but translated into 15 languages.\n\nThus, this model has been specifically trained for the task of natural language inference across several datasets and multiple languages. It should be especially useful for this task, particularly in a multilingual context.","metadata":{}},{"cell_type":"markdown","source":"## Quick Setup","metadata":{}},{"cell_type":"code","source":"!pip install evaluate # library for metrics","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-12-02T09:31:38.131680Z","iopub.execute_input":"2024-12-02T09:31:38.131983Z","iopub.status.idle":"2024-12-02T09:31:46.271167Z","shell.execute_reply.started":"2024-12-02T09:31:38.131960Z","shell.execute_reply":"2024-12-02T09:31:46.270018Z"},"_kg_hide-input":true,"trusted":true},"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.23.5)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.5.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.65.0)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.2.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.14)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2023.6.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.16.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.8.4)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (0.18.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2023.5.7)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\nInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"import evaluate\nimport torch\nfrom transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer","metadata":{"execution":{"iopub.status.busy":"2024-12-02T09:31:52.625955Z","iopub.execute_input":"2024-12-02T09:31:52.626294Z","iopub.status.idle":"2024-12-02T09:31:58.607775Z","shell.execute_reply.started":"2024-12-02T09:31:52.626265Z","shell.execute_reply":"2024-12-02T09:31:58.607110Z"},"trusted":true},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"## Tokenizer","metadata":{}},{"cell_type":"markdown","source":"A tokenizer is a component that breaks text into individual \"tokens\". In the context of Natural Language Processing (NLP), a token typically represents a word or symbol. For example, the sentence \"Hello, world!\" might be tokenized into: [\"Hello\", \",\", \"world\", \"!\"].\n\nTokenization is the first step in most NLP tasks, including tasks involving Transformer models such as BERT, GPT-2, XLM-RoBERTa, etc. These models are trained on tokenized texts and operate on tokenized input data.\n\nThe specific tokenizer used with a particular model is typically trained alongside the model and knows how to properly break text into tokens in the way that was used during the model's training. Each token is then associated with a unique numerical identifier that the model uses for training and inference.\n\nIt's important to use the correct tokenizer for your specific model, as different models might use different tokenization schemes. For example, some models might break words into subwords or characters, while others might use whole words as tokens. A mismatch between the tokenization scheme during training and the tokenization scheme during inference can lead to incorrect results.","metadata":{}},{"cell_type":"code","source":"# model_name = 'bert-base-multilingual-cased'\nmodel_name = 'symanto/xlm-roberta-base-snli-mnli-anli-xnli'\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2024-12-02T09:32:06.325439Z","iopub.execute_input":"2024-12-02T09:32:06.326126Z","iopub.status.idle":"2024-12-02T09:32:08.448622Z","shell.execute_reply.started":"2024-12-02T09:32:06.326095Z","shell.execute_reply":"2024-12-02T09:32:08.447898Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer_config.json:   0%|          | 0.00/398 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1acad3594bd74b739e093fc774fc0459"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)tencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8f394480f8b407681d832e497775ea8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4e914386b1a47c2bad5f45173b87b73"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f6bf1d8533e4300a2e66a211864f48f"}},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T09:32:14.013928Z","iopub.execute_input":"2024-12-02T09:32:14.014695Z","iopub.status.idle":"2024-12-02T09:32:14.025804Z","shell.execute_reply.started":"2024-12-02T09:32:14.014662Z","shell.execute_reply":"2024-12-02T09:32:14.024982Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"   pair_id  premise_id                                            premise  \\\n0   101315       10131  Presiden Joko Widodo (Jokowi) menyampaikan pre...   \n1   110511       11051  Meski biasanya hanya digunakan di fasilitas ke...   \n2   124434       12443  Data dari Nielsen Music mencatat, \"Joanne\" tel...   \n3   124274       12427  Album Wild West miliknya pada tahun 1981 merup...   \n4   119442       11944  Seperti namanya, paket internet sahur Telkomse...   \n\n                                          hypothesis  label data_split  \\\n0     Prediksi akhir wabah tidak disampaikan Jokowi.      2      train   \n1  Masker sekali pakai banyak dipakai di tingkat ...      0      train   \n2      Nielsen Music mencatat pada akhir minggu ini.      1      train   \n3                 Ia memiliki lebih dari satu album.      1      train   \n4  Paket internet sahur tidak ditujukan untuk saa...      2      train   \n\n  annotator_type sentence_size  \n0            lay        single  \n1            lay        single  \n2            lay        single  \n3            lay        single  \n4            lay        single  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pair_id</th>\n      <th>premise_id</th>\n      <th>premise</th>\n      <th>hypothesis</th>\n      <th>label</th>\n      <th>data_split</th>\n      <th>annotator_type</th>\n      <th>sentence_size</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>101315</td>\n      <td>10131</td>\n      <td>Presiden Joko Widodo (Jokowi) menyampaikan pre...</td>\n      <td>Prediksi akhir wabah tidak disampaikan Jokowi.</td>\n      <td>2</td>\n      <td>train</td>\n      <td>lay</td>\n      <td>single</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>110511</td>\n      <td>11051</td>\n      <td>Meski biasanya hanya digunakan di fasilitas ke...</td>\n      <td>Masker sekali pakai banyak dipakai di tingkat ...</td>\n      <td>0</td>\n      <td>train</td>\n      <td>lay</td>\n      <td>single</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>124434</td>\n      <td>12443</td>\n      <td>Data dari Nielsen Music mencatat, \"Joanne\" tel...</td>\n      <td>Nielsen Music mencatat pada akhir minggu ini.</td>\n      <td>1</td>\n      <td>train</td>\n      <td>lay</td>\n      <td>single</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>124274</td>\n      <td>12427</td>\n      <td>Album Wild West miliknya pada tahun 1981 merup...</td>\n      <td>Ia memiliki lebih dari satu album.</td>\n      <td>1</td>\n      <td>train</td>\n      <td>lay</td>\n      <td>single</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>119442</td>\n      <td>11944</td>\n      <td>Seperti namanya, paket internet sahur Telkomse...</td>\n      <td>Paket internet sahur tidak ditujukan untuk saa...</td>\n      <td>2</td>\n      <td>train</td>\n      <td>lay</td>\n      <td>single</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"# Delete unnecessary columns for indonli\n\ntrain = train.drop(labels=['premise_id','pair_id','data_split','annotator_type','sentence_size'], axis=1)\n\nprint(train.columns)\ntest = test.drop(labels=['language','lang_abv'], axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T09:32:18.054298Z","iopub.execute_input":"2024-12-02T09:32:18.054602Z","iopub.status.idle":"2024-12-02T09:32:18.065133Z","shell.execute_reply.started":"2024-12-02T09:32:18.054580Z","shell.execute_reply":"2024-12-02T09:32:18.064111Z"}},"outputs":[{"name":"stdout","text":"Index(['premise', 'hypothesis', 'label'], dtype='object')\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T09:32:22.245639Z","iopub.execute_input":"2024-12-02T09:32:22.246354Z","iopub.status.idle":"2024-12-02T09:32:22.254830Z","shell.execute_reply.started":"2024-12-02T09:32:22.246327Z","shell.execute_reply":"2024-12-02T09:32:22.253896Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"                                             premise  \\\n0  Presiden Joko Widodo (Jokowi) menyampaikan pre...   \n1  Meski biasanya hanya digunakan di fasilitas ke...   \n2  Data dari Nielsen Music mencatat, \"Joanne\" tel...   \n3  Album Wild West miliknya pada tahun 1981 merup...   \n4  Seperti namanya, paket internet sahur Telkomse...   \n\n                                          hypothesis  label  \n0     Prediksi akhir wabah tidak disampaikan Jokowi.      2  \n1  Masker sekali pakai banyak dipakai di tingkat ...      0  \n2      Nielsen Music mencatat pada akhir minggu ini.      1  \n3                 Ia memiliki lebih dari satu album.      1  \n4  Paket internet sahur tidak ditujukan untuk saa...      2  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>premise</th>\n      <th>hypothesis</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Presiden Joko Widodo (Jokowi) menyampaikan pre...</td>\n      <td>Prediksi akhir wabah tidak disampaikan Jokowi.</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Meski biasanya hanya digunakan di fasilitas ke...</td>\n      <td>Masker sekali pakai banyak dipakai di tingkat ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Data dari Nielsen Music mencatat, \"Joanne\" tel...</td>\n      <td>Nielsen Music mencatat pada akhir minggu ini.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Album Wild West miliknya pada tahun 1981 merup...</td>\n      <td>Ia memiliki lebih dari satu album.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Seperti namanya, paket internet sahur Telkomse...</td>\n      <td>Paket internet sahur tidak ditujukan untuk saa...</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"# delete unnecessary columns\ntrain = train.drop(labels=['language', 'text_length', 'lang_abv'], axis=1)\ntest = test.drop(labels=['language','lang_abv'], axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> Hugging Face have classes DatasetDict() and Dataset(). They convert data into a format convenient for the model","metadata":{}},{"cell_type":"code","source":"from datasets import Dataset, DatasetDict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T09:32:30.445389Z","iopub.execute_input":"2024-12-02T09:32:30.445731Z","iopub.status.idle":"2024-12-02T09:32:30.449922Z","shell.execute_reply.started":"2024-12-02T09:32:30.445705Z","shell.execute_reply":"2024-12-02T09:32:30.449105Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_df, val_df = train_test_split(train, test_size=0.2, random_state=42)\n\ntrain_ds = Dataset.from_pandas(train_df)\nval_ds = Dataset.from_pandas(val_df)\ntest_ds = Dataset.from_pandas(test)\n\nds = DatasetDict()\nds['train'] = train_ds\nds['validation'] = val_ds\nds['test'] = test_ds\n\nds","metadata":{"execution":{"iopub.status.busy":"2024-12-02T09:32:33.214190Z","iopub.execute_input":"2024-12-02T09:32:33.214527Z","iopub.status.idle":"2024-12-02T09:32:33.275864Z","shell.execute_reply.started":"2024-12-02T09:32:33.214500Z","shell.execute_reply":"2024-12-02T09:32:33.275136Z"},"trusted":true},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['premise', 'hypothesis', 'label', '__index_level_0__'],\n        num_rows: 8264\n    })\n    validation: Dataset({\n        features: ['premise', 'hypothesis', 'label', '__index_level_0__'],\n        num_rows: 2066\n    })\n    test: Dataset({\n        features: ['id', 'premise', 'hypothesis'],\n        num_rows: 5195\n    })\n})"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"# tokenaize sentence func\ndef tokenizer_sentence(data):\n    return tokenizer(data['premise'], data['hypothesis'], truncation=True)","metadata":{"execution":{"iopub.status.busy":"2024-12-02T09:32:37.466839Z","iopub.execute_input":"2024-12-02T09:32:37.467962Z","iopub.status.idle":"2024-12-02T09:32:37.473128Z","shell.execute_reply.started":"2024-12-02T09:32:37.467918Z","shell.execute_reply":"2024-12-02T09:32:37.472140Z"},"trusted":true},"outputs":[],"execution_count":24},{"cell_type":"code","source":"tokenized_ds = ds.map(tokenizer_sentence, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-12-02T09:32:41.315716Z","iopub.execute_input":"2024-12-02T09:32:41.316050Z","iopub.status.idle":"2024-12-02T09:32:43.031668Z","shell.execute_reply.started":"2024-12-02T09:32:41.316023Z","shell.execute_reply":"2024-12-02T09:32:43.030855Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/9 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ea8d448d7e3492e9de0bf04dc2ba377"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"443783181e0547288e4b7743c2e94fe3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/6 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cc118d28f7a41a988e12d8cb356cf55"}},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"tokenized_ds","metadata":{"execution":{"iopub.status.busy":"2024-12-02T09:32:45.964505Z","iopub.execute_input":"2024-12-02T09:32:45.964814Z","iopub.status.idle":"2024-12-02T09:32:45.970334Z","shell.execute_reply.started":"2024-12-02T09:32:45.964792Z","shell.execute_reply":"2024-12-02T09:32:45.969358Z"},"trusted":true},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['premise', 'hypothesis', 'label', '__index_level_0__', 'input_ids', 'attention_mask'],\n        num_rows: 8264\n    })\n    validation: Dataset({\n        features: ['premise', 'hypothesis', 'label', '__index_level_0__', 'input_ids', 'attention_mask'],\n        num_rows: 2066\n    })\n    test: Dataset({\n        features: ['id', 'premise', 'hypothesis', 'input_ids', 'attention_mask'],\n        num_rows: 5195\n    })\n})"},"metadata":{}}],"execution_count":26},{"cell_type":"markdown","source":"> As we can see, the columns 'input_ids', 'attention_mask' have been added","metadata":{}},{"cell_type":"markdown","source":"> DataCollatorWithPadding complements or truncates data to a fixed length.","metadata":{}},{"cell_type":"code","source":"data_collator = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-12-02T09:32:55.957101Z","iopub.execute_input":"2024-12-02T09:32:55.957915Z","iopub.status.idle":"2024-12-02T09:32:55.961477Z","shell.execute_reply.started":"2024-12-02T09:32:55.957887Z","shell.execute_reply":"2024-12-02T09:32:55.960615Z"},"trusted":true},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"## Build Model","metadata":{}},{"cell_type":"markdown","source":"Let's create the custom class named `CustomXLMRobertaModel` that extends the `nn.Module` class from PyTorch, which means it represents a neural network module.\n\n- `__init__(self, num_labels)`: This method initializes the class instance. It takes `num_labels` as an argument, which represents the number of possible output labels (classes) for the model.\n\n- `super(CustomXLMRobertaModel, self).__init__()`: This is calling the `__init__` method of the parent `nn.Module` class, which is necessary to properly initialize the class.\n\n- `model_name = 'symanto/xlm-roberta-base-snli-mnli-anli-xnli'`: This specifies the pre-trained model to use. In this case, it is a pre-trained XLM-RoBERTa model.\n\n- `self.roberta = XLMRobertaModel.from_pretrained(model_name)`: This loads the specified pre-trained XLM-RoBERTa model.\n\n- `self.dropout = nn.Dropout(0.2)`: This is a dropout layer, which is a regularization technique that helps prevent overfitting. The `0.2` specifies that approximately 20% of the inputs will be randomly set to 0 during training.\n\n- `self.classifier = nn.Sequential(...)`: This is the classification layer of the model, which takes the output from the XLM-RoBERTa model and produces the final class predictions. It consists of a sequence of operations (a linear transformation, layer normalization, a ReLU activation function, another dropout, and another linear transformation).\n\n- `self.loss = nn.CrossEntropyLoss()`: This specifies the loss function to be used, which is cross entropy loss. This is a common choice for multi-class classification tasks.\n\n- `self.num_labels = num_labels`: This just stores the number of possible output labels for later use.\n\n- `def forward(self, input_ids, attention_mask, labels=None)`: This is the method that is called when you pass input data to the model. It takes as input the `input_ids` (the tokenized input data), the `attention_mask` (which specifies which tokens should be attended to by the model), and optionally the true `labels`.\n\n- The `output` is obtained by passing the `input_ids` and `attention_mask` to the XLM-RoBERTa model, then passing the resulting `pooler_output` through the dropout layer.\n\n- The `logits` are obtained by passing the `output` through the classification layer. These are the raw, unnormalized scores for each class.\n\n- If `labels` are provided, then it calculates the cross entropy loss between the predicted `logits` and the true `labels`, and returns a dictionary containing both the `loss` and the `logits`. If `labels` are not provided, it simply returns the `logits`.","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nfrom transformers import XLMRobertaModel\n\nclass CustomXLMRobertaModel(nn.Module):\n    def __init__(self, num_labels):\n        super(CustomXLMRobertaModel, self).__init__()\n        model_name = 'symanto/xlm-roberta-base-snli-mnli-anli-xnli'\n        self.roberta = XLMRobertaModel.from_pretrained(model_name)\n        self.dropout = nn.Dropout(0.2)\n        self.classifier = nn.Sequential(\n            nn.Linear(768, 512),\n            nn.LayerNorm(512),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(512, num_labels)\n        )\n        self.loss = nn.CrossEntropyLoss()\n        self.num_labels = num_labels\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        output = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n        output = self.dropout(output.pooler_output)\n        logits = self.classifier(output)\n\n        if labels is not None:\n            loss = self.loss(logits.view(-1, self.num_labels), labels.view(-1))\n            return {\"loss\": loss, \"logits\": logits}\n        else:\n            return logits","metadata":{"execution":{"iopub.status.busy":"2024-12-02T09:33:03.421380Z","iopub.execute_input":"2024-12-02T09:33:03.422140Z","iopub.status.idle":"2024-12-02T09:33:03.440937Z","shell.execute_reply.started":"2024-12-02T09:33:03.422109Z","shell.execute_reply":"2024-12-02T09:33:03.440282Z"},"trusted":true},"outputs":[],"execution_count":28},{"cell_type":"code","source":"model = CustomXLMRobertaModel(num_labels=3) # we have 3 classes","metadata":{"execution":{"iopub.status.busy":"2024-12-02T09:33:10.727850Z","iopub.execute_input":"2024-12-02T09:33:10.728583Z","iopub.status.idle":"2024-12-02T09:33:28.071607Z","shell.execute_reply.started":"2024-12-02T09:33:10.728553Z","shell.execute_reply":"2024-12-02T09:33:28.070767Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading config.json:   0%|          | 0.00/921 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f9fadeb38a243d3b78bd65a25edcf36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/1.11G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"971a076d7055496dae310bd51cb249c3"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at symanto/xlm-roberta-base-snli-mnli-anli-xnli were not used when initializing XLMRobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of XLMRobertaModel were not initialized from the model checkpoint at symanto/xlm-roberta-base-snli-mnli-anli-xnli and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"## Train Model","metadata":{}},{"cell_type":"markdown","source":"`TrainingArguments` is a class from the `transformers` library that provides arguments and parameters for training a model. This function takes a multitude of arguments that allow the training process to be configured.\n\nHere's what each of the provided parameters does:\n\n1. `output_dir` (\"/content\" in your case): This is the path to the directory where output files, such as the trained model and logs, will be saved.\n\n2. `optim` (\"adamw_torch\"): This is the optimizer that will be used to update the model's weights during training. \"adamw_torch\" is a variant of the Adam optimization algorithm that incorporates weight decay, often used to regularize the model and prevent overfitting.\n\n3. `num_train_epochs` (5): This is the number of epochs the training will run for. One epoch means one complete pass through the entire training dataset.\n\n4. `evaluation_strategy` (\"epoch\"): This is the strategy for evaluating the model. If set to \"epoch\", the model will be evaluated after each training epoch. Other possible values include \"steps\" (evaluate after a given number of training steps) and \"no\" (no evaluation).\n\n5. `logging_dir` ('./logs'): This is the path to the directory where training process logs will be saved.\n\n6. `logging_steps` (10): This is the number of training steps between log entries. If set to 10, a log entry will be created every 10 training steps.\n\nThere are many other parameters can also be set in `TrainingArguments`. You found it in the [documentation](https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments).","metadata":{}},{"cell_type":"markdown","source":"> Logs. What's it?\n\nTraining process logs are records that capture key information about the progress of a model's training. This can include data about each step or epoch of training, such as training losses, quality metrics (accuracy, F1 score, etc.), hyperparameter values, training time, and other useful information.\n\nTraining logs are used for the following reasons:\n\n1. **Monitoring Progress**: Training logs allow for tracking the model's training progress over time and identifying when the model begins to overfit or when the training stabilizes.\n\n2. **Debugging and Optimization**: If the training process is not going as expected, logs can help identify issues and determine how to optimize the process. For example, if training losses suddenly increase, this might indicate convergence problems.\n\n3. **Record-keeping and Reproducibility**: Saving training logs allows for keeping a record of how the model was trained, which is important for reproducibility of scientific results. This can also be useful when comparing different models or training strategies.\n\n4. **Visualization**: Training logs can be used to create graphs and charts that visualize the training progress. This can be especially helpful in analyzing and comparing models.\n\nSome tools, such as TensorBoard or Wandb, can automatically visualize these logs, making the process of analysis and monitoring even more convenient.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score\nfrom datasets import load_metric\n\ntraining_args = TrainingArguments(\"/content\",\n                                  optim=\"adamw_torch\",\n                                  learning_rate=3e-5,\n                                  per_device_train_batch_size=16,                                \n                                  num_train_epochs=10,\n                                  evaluation_strategy=\"epoch\",\n                                  logging_dir='./logs',\n                                  logging_steps=10)\n\nf1_metric = load_metric(\"f1\")\n\ndef compute_metrics(eval_preds):\n    logits, labels = eval_preds\n    predictions = np.argmax(logits, axis=-1)\n    return {\n        'accuracy': accuracy_score(labels, predictions),\n        'f1': f1_metric.compute(predictions=predictions, references=labels, average=\"micro\")\n    }","metadata":{"execution":{"iopub.status.busy":"2024-12-02T09:33:50.348469Z","iopub.execute_input":"2024-12-02T09:33:50.349330Z","iopub.status.idle":"2024-12-02T09:33:54.089611Z","shell.execute_reply.started":"2024-12-02T09:33:50.349300Z","shell.execute_reply":"2024-12-02T09:33:54.088892Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.06k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09ccd05004d640f596e7a0bcdb8a46ff"}},"metadata":{}}],"execution_count":30},{"cell_type":"markdown","source":"`Trainer` provides a straightforward and fast way to train and evaluate your model.\n\n`trainer = Trainer(...`: This creates an instance of the `Trainer` class, which will be used for training and evaluating the model. The arguments passed are:\n\n   - `model`: this is the model to be trained. In this context, it isn't defined yet, but it could be any model compatible with the transformers library.\n\n   - `args=training_args`: these arguments control the training process. `training_args` should be an instance of `TrainingArguments` or compatible, which defines parameters like the learning rate, batch size, etc.\n\n   - `train_dataset=tokenized_ds[\"train\"]` and `eval_dataset=tokenized_ds[\"validation\"]`: these are the datasets for training and evaluating the model. In this case, they are taken from the dictionary `tokenized_ds`, presumably containing tokenized versions of the original data.\n\n   - `data_collator=data_collator`: the `data_collator` is a function that takes a list of samples from the `Dataset` and collates them into mini-batches (batches) for training or evaluation. It isn't defined in this particular context.\n\n   - `tokenizer=tokenizer`: this is the tokenizer that was used to tokenize the input data.\n\n   - `compute_metrics=compute_metrics`: this is a function that will be used to compute metrics during evaluation. In this context, it isn't defined yet. It should take the outputs from `Trainer.evaluate()` and return a dictionary where keys are the names of the metrics and values are the computed metrics.","metadata":{}},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(\n    model,\n    args=training_args,\n    train_dataset=tokenized_ds[\"train\"],\n    eval_dataset=tokenized_ds[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,  # передаем функцию compute_metrics\n)","metadata":{"execution":{"iopub.status.busy":"2024-12-02T09:34:03.757912Z","iopub.execute_input":"2024-12-02T09:34:03.758631Z","iopub.status.idle":"2024-12-02T09:34:03.766381Z","shell.execute_reply.started":"2024-12-02T09:34:03.758599Z","shell.execute_reply":"2024-12-02T09:34:03.765610Z"},"trusted":true},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":"### Setup env","metadata":{}},{"cell_type":"markdown","source":"Wandb, or Weights & Biases, is a machine learning tool that helps to track and visualize the progress of model learning, as well as compare various experiments. It provides a convenient web interface where you can observe your experiments in real time, see graphs of metrics such as loss and accuracy, save and load model weights, and share the results with colleagues.\n\nString `os.environ[\"WANDB_DISABLED\"] = \"true\"` disables Weights & Biases integration. This can be useful if you don't want your experiments to be blocked in Wandb, or if you work in an environment where internet access is limited or unavailable. ","metadata":{}},{"cell_type":"code","source":"!pip install wandb","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-12-02T09:34:07.443039Z","iopub.execute_input":"2024-12-02T09:34:07.443513Z","iopub.status.idle":"2024-12-02T09:34:15.187825Z","shell.execute_reply.started":"2024-12-02T09:34:07.443466Z","shell.execute_reply":"2024-12-02T09:34:15.186927Z"},"trusted":true},"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRequirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.15.5)\nRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.3)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.31)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.27.1)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0)\nRequirement already satisfied: pathtools in /opt/conda/lib/python3.10/site-packages (from wandb) (0.1.2)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (59.8.0)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2023.5.7)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"execution":{"iopub.status.busy":"2024-12-02T09:34:20.059339Z","iopub.execute_input":"2024-12-02T09:34:20.059689Z","iopub.status.idle":"2024-12-02T09:34:20.064786Z","shell.execute_reply.started":"2024-12-02T09:34:20.059659Z","shell.execute_reply":"2024-12-02T09:34:20.063830Z"},"trusted":true},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":"### Start train process","metadata":{}},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-12-02T09:34:23.799626Z","iopub.execute_input":"2024-12-02T09:34:23.799936Z","iopub.status.idle":"2024-12-02T10:06:12.715886Z","shell.execute_reply.started":"2024-12-02T09:34:23.799911Z","shell.execute_reply":"2024-12-02T10:06:12.714520Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.5"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."},"metadata":{}},{"name":"stderr","text":"You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning:\n\nWas asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2590' max='2590' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2590/2590 31:10, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.548600</td>\n      <td>0.533522</td>\n      <td>0.801549</td>\n      <td>{'f1': 0.8015488867376573}</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.435700</td>\n      <td>0.545013</td>\n      <td>0.801549</td>\n      <td>{'f1': 0.8015488867376573}</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.395300</td>\n      <td>0.589025</td>\n      <td>0.795257</td>\n      <td>{'f1': 0.7952565343659246}</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.317100</td>\n      <td>0.669074</td>\n      <td>0.792836</td>\n      <td>{'f1': 0.7928363988383349}</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.167300</td>\n      <td>0.829791</td>\n      <td>0.796225</td>\n      <td>{'f1': 0.7962245885769604}</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.176700</td>\n      <td>0.952783</td>\n      <td>0.792352</td>\n      <td>{'f1': 0.7923523717328169}</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.136200</td>\n      <td>1.102883</td>\n      <td>0.789932</td>\n      <td>{'f1': 0.7899322362052275}</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.099000</td>\n      <td>1.176610</td>\n      <td>0.796709</td>\n      <td>{'f1': 0.7967086156824781}</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.023700</td>\n      <td>1.246914</td>\n      <td>0.797677</td>\n      <td>{'f1': 0.797676669893514}</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.059800</td>\n      <td>1.271758</td>\n      <td>0.801065</td>\n      <td>{'f1': 0.8010648596321394}</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Trainer is attempting to log a value of \"{'f1': 0.8015488867376573}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning:\n\nWas asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning:\n\nWas asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n\nTrainer is attempting to log a value of \"{'f1': 0.8015488867376573}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning:\n\nWas asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n\nTrainer is attempting to log a value of \"{'f1': 0.7952565343659246}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning:\n\nWas asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning:\n\nWas asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n\nTrainer is attempting to log a value of \"{'f1': 0.7928363988383349}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning:\n\nWas asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n\nTrainer is attempting to log a value of \"{'f1': 0.7962245885769604}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning:\n\nWas asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning:\n\nWas asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n\nTrainer is attempting to log a value of \"{'f1': 0.7923523717328169}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning:\n\nWas asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n\nTrainer is attempting to log a value of \"{'f1': 0.7899322362052275}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning:\n\nWas asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning:\n\nWas asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n\nTrainer is attempting to log a value of \"{'f1': 0.7967086156824781}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning:\n\nWas asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n\nTrainer is attempting to log a value of \"{'f1': 0.797676669893514}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning:\n\nWas asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning:\n\nWas asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n\nTrainer is attempting to log a value of \"{'f1': 0.8010648596321394}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","output_type":"stream"},{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=2590, training_loss=0.22148267769013824, metrics={'train_runtime': 1908.5257, 'train_samples_per_second': 43.3, 'train_steps_per_second': 1.357, 'total_flos': 0.0, 'train_loss': 0.22148267769013824, 'epoch': 10.0})"},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"# In kaggle, there is no way to access tensorboard \n# but you can use this code to visualize learning graphs from logs.\n\n# !pip install tensorboard\n# %load_ext tensorboard\n\n# %tensorboard --logdir /kaggle/working/logs (your path to logs dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Get Model predictions","metadata":{}},{"cell_type":"code","source":"predictions = trainer.predict(tokenized_ds[\"test\"])\npredictions","metadata":{"execution":{"iopub.status.busy":"2024-12-01T08:35:33.100894Z","iopub.execute_input":"2024-12-01T08:35:33.101266Z","iopub.status.idle":"2024-12-01T08:36:19.740806Z","shell.execute_reply.started":"2024-12-01T08:35:33.101237Z","shell.execute_reply":"2024-12-01T08:36:19.739644Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"PredictionOutput(predictions=array([[-2.737657 , -2.203037 ,  4.2112136],\n       [-2.381342 ,  4.047823 , -2.3333812],\n       [ 4.660001 , -1.9491919, -2.1156757],\n       ...,\n       [ 4.5528708, -2.0941494, -1.8590173],\n       [ 0.6248544,  1.742324 , -1.9893024],\n       [-2.826497 , -2.1592557,  4.293371 ]], dtype=float32), label_ids=None, metrics={'test_runtime': 46.6276, 'test_samples_per_second': 111.415, 'test_steps_per_second': 6.97})"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"logits = torch.from_numpy(predictions.predictions)\nprobs = torch.softmax(logits, -1).tolist() # convert to probability\nprobs[:5]","metadata":{"execution":{"iopub.status.busy":"2024-12-01T08:40:25.076560Z","iopub.execute_input":"2024-12-01T08:40:25.077300Z","iopub.status.idle":"2024-12-01T08:40:25.094314Z","shell.execute_reply.started":"2024-12-01T08:40:25.077268Z","shell.execute_reply":"2024-12-01T08:40:25.093551Z"},"trusted":true},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"[[0.0009572316776029766, 0.0016338031273335218, 0.997408926486969],\n [0.0016084787202998996, 0.9967039227485657, 0.0016875024884939194],\n [0.9975171089172363, 0.0013445729855448008, 0.00113836454693228],\n [0.001298150629736483, 0.9972037076950073, 0.0014981417916715145],\n [0.0012291505699977279, 0.9970943927764893, 0.0016764780739322305]]"},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"outputs = []\n\nfor index, prob in enumerate(probs):\n    # ind indx with max probability of class\n    predicted_label = prob.index(max(prob))\n    element_id = ds['test']['id'][index]\n    prediction = (element_id, predicted_label)\n    outputs.append(prediction)","metadata":{"execution":{"iopub.status.busy":"2024-12-01T08:40:30.284732Z","iopub.execute_input":"2024-12-01T08:40:30.285415Z","iopub.status.idle":"2024-12-01T08:40:47.099305Z","shell.execute_reply.started":"2024-12-01T08:40:30.285382Z","shell.execute_reply":"2024-12-01T08:40:47.098271Z"},"trusted":true},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":"## Save Submision","metadata":{}},{"cell_type":"code","source":"submission = pd.DataFrame(outputs, columns=['id', 'prediction'])\nsubmission.to_csv(\"submission.csv\", index=False)\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2024-12-01T08:43:16.775783Z","iopub.execute_input":"2024-12-01T08:43:16.776633Z","iopub.status.idle":"2024-12-01T08:43:16.805041Z","shell.execute_reply.started":"2024-12-01T08:43:16.776603Z","shell.execute_reply":"2024-12-01T08:43:16.804080Z"},"trusted":true},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"           id  prediction\n0  c6d58c3f69           2\n1  cefcc82292           1\n2  e98005252c           0\n3  58518c10ba           1\n4  c32b0d16df           1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>c6d58c3f69</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>cefcc82292</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>e98005252c</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>58518c10ba</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>c32b0d16df</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":34}]}